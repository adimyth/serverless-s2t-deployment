{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d518c6e4",
      "metadata": {
        "id": "d518c6e4"
      },
      "source": [
        "# AI4Bharat ASR to HF Compatible Format\n",
        "\n",
        "The objective of this notebook is to convert the AI4Bharat ASR models to the Hugging Face compatible \"transformers\" format. This allows us to use transformer's \"automatic-speech-recognition\" pipeline to transcribe speech using the AI4Bharat models.\n",
        "\n",
        "This notebook focuses on converting the [indicwav2vec-kannada](https://github.com/AI4Bharat/IndicWav2Vec?tab=readme-ov-file#download-models) model to the Hugging Face compatible format. The same steps can be followed for other AI4Bharat ASR models.\n",
        "\n",
        "You can run this on Google Colab or any other environment with a GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f511fa0b",
      "metadata": {
        "id": "f511fa0b"
      },
      "source": [
        "## Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1a7a4211",
      "metadata": {
        "id": "1a7a4211",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "5692ad30-74e5-40d0-93f9-633fb0ca413f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libbz2-dev is already the newest version (1.0.8-5build1).\n",
            "libbz2-dev set to manually installed.\n",
            "liblzma-dev is already the newest version (5.2.5-2ubuntu1).\n",
            "liblzma-dev set to manually installed.\n",
            "libboost-all-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu9.2).\n",
            "zlib1g-dev set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Repository: 'deb https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu/ jammy main'\n",
            "Description:\n",
            "FFmpeg 4.4.5 builds (& associated multimedia packages) for Xenial & newer.\n",
            "\n",
            "*** Anyone interested in full builds of FFmpeg 4.4.x including all \"bells and whistles\" needs to have donated, after which access to the new private PPA can be requested. See my Launchpad page linked below for details. ***\n",
            "\n",
            "*** Please always see https://launchpad.net/~savoury1 for general updates about this Launchpad site before contacting me or reporting any bugs! ***\n",
            "\n",
            "*** Big thanks to all those who have donated to support this project, you are very directly helping to keep it alive! To all who have not donated: please do so if you can afford it, this project depends on donations. ***\n",
            "\n",
            "If software at this site is useful to you then please consider a donation:\n",
            "\n",
            "*** Donations: https://paypal.me/Savoury1 & https://ko-fi.com/Savoury1 ***\n",
            "*** Also https://patreon.com/Savoury1 & https://liberapay.com/Savoury1 ***\n",
            "\n",
            "==========================================================================\n",
            "                   FFmpeg 4 - media tool (Xenial & newer)\n",
            "==========================================================================\n",
            "\n",
            "abseil (20240116.2), alsa-{lib,plugins,tools,topology-conf,ucm-conf,utils} (1.2.{5,6}), aom (3.9.1), aribb24 (1.0.3+git20160115), chafa (1.14.0), chromaprint (1.5.1), cjson (1.7.17), codec2 (1.2.0), cppzmq (4.10.0), dav1d (1.4.3), ffmpeg (4.4.5), ffms2 (2.23), flac (1.4.3), fluidsynth (2.3.5), fontconfig (2.13.1), freeglut (3.4.0), freetype (2.11.1), glib2.0 (2.72.4), gmp (6.3.0), gnutls28 (3.7.3), gobject-introspection (1.72.0), gpac (2.2.1), gst-{libav,python}1.0 (1.24.{5,6,7} for gst*), gst-plugins-{bad,base,good,ugly}1.0, gst-plugins-rtp (1.18.1.2), gstreamer{-vaapi,1.0}, harfbuzz (8.3.0), ilmbase (3.1.11 = imath), intel-gmmlib (22.4.1), intel-media-driver{,-non-free} (24.2.5), intel-mediasdk (23.2.2), json-glib (1.6.6), lensfun (0.3.3), libass (0.17.3), libavtp (0.2.0), libbluray (1.3.4), libbs2b (3.1.0), libcamera (0.3.1), libcdio (2.1.0), libcdio-paranoia (10.2+2.0.1), libdeflate (1.20), libdvdnav (6.1.1), libdvdread (6.1.3), libevent (2.1.12-stable), libffado (2.4.9), libfreeaptx (0.1.1), libgcrypt20 (1.10.3), libgsm (1.0.22), libinstpatch (1.1.6), liblc3 (1.1.1), libldac (2.0.2.3+git20200429), libmysofa (1.3.2), libnice (0.1.21), libopenaptx (0.2.0), libopenmpt (0.7.8), libpng1.6 (1.6.43), librabbitmq (0.11.0), libsdl2 (2.30.7), libshout (2.4.6), libsodium (1.0.21~git20240801), libsoup3 (3.1.4), libsrtp2 (2.5.0), libssh (0.10.6), libtasn1-6 (4.18.0), libudfread (1.1.2), libunistring (1.0), libunwind (1.3.2), libva{-utils} (2.22.0), libvpx (1.14.1), libwebm (1.0.0.31), libwebp (1.4.0), libxfixes (6.0.0), libyaml (0.2.5), libyuv (0.0.1888.20240522), lua5.4 (5.4.6), mbedtls (2.28.8), mpg123 (1.32.5), nettle (3.7.3), norm (1.5.9), openal-soft (1.22.2), opencore-amr (0.1.6), openexr (3.2.4), openfec (1.4.2.10), openh264 (2.4.1), openjpeg2 (2.5.0), opus (1.5.2), orc (0.4.38), pipewire (1.2.3), pipewire-media-session (0.4.2), qpwgraph (0.7.7), roc-toolkit (0.3.0), rubberband (3.1.2), sbc (2.0), snappy (1.1.10), sndio (1.9.0), soundtouch (2.3.2), speex{dsp} (1.2.1), srt (1.5.3), timgm6mb-soundfont, unbound (1.17.1), ust (2.13.4), vulkan-{headers,loader,tools} (1.3.283.0), wavpack (5.7.0), webrtc-audio-processing (1.3), wireplumber (0.5.5), x264 (0.164.3191 = libx264-164), x265 (3.5 = libx265-199), zeromq3 (4.3.5), zimg (3.0.5), zlib (1.3.1), zvbi (0.2.42), zxing-cpp (2.2.1)\n",
            "\n",
            "Focal & Jammy: libdecor-0 (0.2.2 for libsdl2 >= 2.0.20)\n",
            "\n",
            "Focal only: autogen (= abseil 20240116.2 / renamed source for i386), gnome-keyring (3.36.0 rebuild for compat with new GLib), libdbl-perl (1.20 = libdeflate / renamed source for i386)\n",
            "\n",
            "Bionic & Focal: snapd-glib (1.60 for PipeWire >= 1.0.1-1ubuntu3~)\n",
            "\n",
            "Xenial & Bionic: alsa-oss (1.1.8), alsa-plugins-extra (1.1.0), alsaequal (0.6), fftw3 (3.3.8), fribidi (1.0.8), gnome-keyring (3.28.2), lcms2 (2.9), leptonlib (1.79.0), libgpg-error, libvidstab (1.1.0), p11-kit, tesseract (4.1.1), wayland-protocols (1.18),  wildmidi (0.4.3)\n",
            "\n",
            "Xenial only: autogen, curl, gcc-7 (7.5.0), lame (3.100), libdrm (2.4.99), libidn2, libpsl, lm-sensors (3.6.0), mesa (18.0.5 rebuild for newer Wayland), nghttp2, openssl, pysimplesoap, python-{boto,httplib2,imaplib2} (compat with newer openssl), readline (7.0), unbound, wayland (1.16.0)\n",
            "\n",
            "==========================================================================\n",
            "\n",
            "*** Install ***\n",
            "\n",
            "FFmpeg can be installed from this PPA alone, as the required packages to satisfy minimum versions have been copied here (notify me if any missing). However, if _all_ newest versions of graphics and multimedia packages are desired then two additional PPAs can be added before installing FFmpeg:\n",
            "\n",
            "  sudo add-apt-repository ppa:savoury1/graphics\n",
            "  sudo add-apt-repository ppa:savoury1/multimedia\n",
            "  sudo add-apt-repository ppa:savoury1/ffmpeg4\n",
            "  sudo apt-get update\n",
            "  sudo apt-get upgrade && sudo apt-get dist-upgrade\n",
            "  sudo apt-get install ffmpeg\n",
            "\n",
            "Notes: GStreamer 1.24.x packages are copied here (ppa:savoury1/multimedia) as FFmpeg and GStreamer must both be built against the Debian SRT package version with renamed libsrt1.5-gnutls or they cannot both be installed at the same time. GStreamer is on basically every Ubuntu-based system so this means it is necessary to upgrade GStreamer packages when upgrading FFmpeg.\n",
            "\n",
            "Also, PipeWire 1.2.x packages are copied here from the PipeWire PPA, due PipeWire now being commonly used by various software. As the FFmpeg 4 PPA is required by numerous PPAs at this Launchpad site it makes the latest PipeWire readily available to all users. PipeWire is built with ALSA 1.2.5 minimum (Jammy, backported to earlier series) which is then required to run, so base ALSA packages are now also copied here for earlier series.\n",
            "\n",
            "GLib will be upgraded to 2.72.4 by adding this PPA as it is required by WirePlumber >= 0.5.0 (and various other new software versions also). This version of GLib causes problems with certain features of GNOME Keyring, so rebuilds of GNOME Keyring for compatibility with GLib >= 2.70.0 are found here as well. These upgrades should be fully backwards compatible and are not known to cause any issues, based on testing a wide range of software.\n",
            "\n",
            "* Xenial systems: As of FFmpeg 4.3 the installation requires GCC >= 7 as the Intel-MediaSDK is enabled, with libmfx1 requiring GCC >= 7 (package fails to build with GCC 5.4.0). So GCC 7.5.0 (ppa:savoury1/toolchain) for Xenial has been copied here, making it easier to install FFmpeg 4.x on Xenial (so adding this FFmpeg 4 PPA will bring a few GCC 7.5.0 upgrades).\n",
            "\n",
            "* Focal systems: Builds of FFmpeg are now also available for i386 whereas they were not available initially. Launchpad does not build most packages for i386 architecture for Focal and newer series (Launchpad only builds i386 packages on a whitelist managed by the Launchpad team) including for dav1d and pocketsphinx, so these features are not enabled for Focal i386.\n",
            "\n",
            "*** Updates ***\n",
            "\n",
            "Update (5 Apr 2023): FFmpeg 4.4 builds from today onwards are now generic, in that they are similar to current Ubuntu 22.04 Jammy FFmpeg builds. All of the dated updates below still apply to the FFmpeg 4.x builds available at a private \"subscriber only\" PPA (ppa:savoury1/ffmpeg) for supporters.\n",
            "\n",
            "Update (24 Jan 2023): FFmpeg 4.4 builds now have AMD AMF (Advanced Media Framework) support, using latest headers from AMF 1.4.29 (released today).\n",
            "\n",
            "Update (23 Oct 2022): FFmpeg 4.4 builds now have rav1e (new Rust-based AV1 encoder) support & are also built with nv-codec-headers 11.1.5.1 (latest).\n",
            "\n",
            "Update (6 Apr 2022): FFmpeg 4.4 builds now have Netflix VMAF support, with thanks to Frank B. (equal #1 patron of these PPAs) for suggesting/testing!\n",
            "\n",
            "Update (8 May 2021): FFmpeg 4.4 uploads from today onwards are built with these additional optional libraries: glslang (shader support), SVT-AV1 (scalable AV1 encoder), zimg (scaling, colorspace conversion, dithering), Vulkan (GPU acceleration on supported hardware), and SMB (Samba sharing).\n",
            "\n",
            "*** Build ***\n",
            "\n",
            "This PPA has build dependencies on:\n",
            "\n",
            "  ppa:savoury1/build-tools\n",
            "  ppa:savoury1/backports\n",
            "  ppa:savoury1/fonts\n",
            "  ppa:savoury1/graphics\n",
            "  ppa:savoury1/multimedia\n",
            "\n",
            "Additionally, for Xenial builds only:\n",
            "\n",
            "  ppa:savoury1/perl-xenial\n",
            "\n",
            "*** Credits ***\n",
            "\n",
            "- Creators of FFmpeg: Michael Niedermayer and the entire FFmpeg team\n",
            "  https://github.com/FFmpeg/FFmpeg/graphs/contributors\n",
            "\n",
            "- Package code: Debian Multimedia Maintainers\n",
            "  https://tracker.debian.org/pkg/ffmpeg\n",
            "More info: https://launchpad.net/~savoury1/+archive/ubuntu/ffmpeg4\n",
            "Adding repository.\n",
            "Adding deb entry to /etc/apt/sources.list.d/savoury1-ubuntu-ffmpeg4-jammy.list\n",
            "Adding disabled deb-src entry to /etc/apt/sources.list.d/savoury1-ubuntu-ffmpeg4-jammy.list\n",
            "Adding key to /etc/apt/trusted.gpg.d/savoury1-ubuntu-ffmpeg4.gpg with fingerprint E996735927E427A733BB653E374C7797FB006459\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [999 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:13 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,573 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,149 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,309 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,224 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,499 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,438 kB]\n",
            "Get:22 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 Packages [96.5 kB]\n",
            "Fetched 19.6 MB in 8s (2,549 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libcodec2-1.0 libmfx1 libsrt1.4-gnutls libva-drm2 libva-x11-2 libva2 libvpx7 libx264-163\n",
            "Use 'apt autoremove' to remove them.\n",
            "The following additional packages will be installed:\n",
            "  libavcodec-dev libavcodec58 libavdevice58 libavfilter7 libavformat-dev libavformat58\n",
            "  libavutil-dev libavutil56 libcodec2-1.2 libfftw3-double3 libpostproc55 librubberband2\n",
            "  libsharpyuv-dev libsharpyuv0 libsnappy1v5 libsndio7 libsndio7.0 libsrt1.5-gnutls\n",
            "  libswresample-dev libswresample3 libswscale-dev libswscale5 libvpx9 libwebp-dev libwebp7\n",
            "  libwebpdecoder3 libwebpdemux2 libwebpmux3 libx264-164 libx265-209\n",
            "Suggested packages:\n",
            "  ffmpeg-doc libcuda1 libnvcuvid1 libnvidia-encode1 libfftw3-bin libfftw3-dev sndiod\n",
            "The following NEW packages will be installed:\n",
            "  libcodec2-1.2 libfftw3-double3 libsharpyuv-dev libsharpyuv0 libsndio7 libsrt1.5-gnutls libvpx9\n",
            "  libwebpdecoder3 libx264-164 libx265-209\n",
            "The following packages will be upgraded:\n",
            "  ffmpeg libavcodec-dev libavcodec58 libavdevice58 libavfilter7 libavformat-dev libavformat58\n",
            "  libavutil-dev libavutil56 libpostproc55 librubberband2 libsnappy1v5 libsndio7.0 libswresample-dev\n",
            "  libswresample3 libswscale-dev libswscale5 libwebp-dev libwebp7 libwebpdemux2 libwebpmux3\n",
            "21 upgraded, 10 newly installed, 0 to remove and 113 not upgraded.\n",
            "Need to get 32.7 MB of archives.\n",
            "After this operation, 42.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfftw3-double3 amd64 3.3.8-2ubuntu8 [770 kB]\n",
            "Get:2 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libavformat-dev amd64 7:4.4.5-0ubuntu1~22.04.sav0 [1,355 kB]\n",
            "Get:3 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libavcodec-dev amd64 7:4.4.5-0ubuntu1~22.04.sav0 [6,004 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libswresample-dev amd64 7:4.4.5-0ubuntu1~22.04.sav0 [96.1 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 ffmpeg amd64 7:4.4.5-0ubuntu1~22.04.sav0 [1,711 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libavdevice58 amd64 7:4.4.5-0ubuntu1~22.04.sav0 [102 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libavfilter7 amd64 7:4.4.5-0ubuntu1~22.04.sav0 [1,453 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libavformat58 amd64 7:4.4.5-0ubuntu1~22.04.sav0 [1,112 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libavcodec58 amd64 7:4.4.5-0ubuntu1~22.04.sav0 [5,391 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libswresample3 amd64 7:4.4.5-0ubuntu1~22.04.sav0 [80.6 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libpostproc55 amd64 7:4.4.5-0ubuntu1~22.04.sav0 [79.0 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libswscale-dev amd64 7:4.4.5-0ubuntu1~22.04.sav0 [222 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libswscale5 amd64 7:4.4.5-0ubuntu1~22.04.sav0 [197 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libavutil-dev amd64 7:4.4.5-0ubuntu1~22.04.sav0 [418 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libavutil56 amd64 7:4.4.5-0ubuntu1~22.04.sav0 [280 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libsndio7.0 amd64 1.9.0-0.3~22.04.sav0 [4,380 B]\n",
            "Get:17 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libsndio7 amd64 1.9.0-0.3~22.04.sav0 [30.5 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 librubberband2 amd64 3.3.0+dfsg-1~22.04.sav0 [128 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libsrt1.5-gnutls amd64 1.5.3-0ubuntu1~22.04.sav0 [326 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libcodec2-1.2 amd64 1.2.0-2~22.04.sav0 [8,997 kB]\n",
            "Get:21 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libsnappy1v5 amd64 1.1.10-1~22.04.sav0 [30.2 kB]\n",
            "Get:22 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libvpx9 amd64 1.14.1-1ubuntu1~22.04.sav0 [1,115 kB]\n",
            "Get:23 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libwebp-dev amd64 1.4.0-0ubuntu1~22.04.sav0 [517 kB]\n",
            "Get:24 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libwebpdemux2 amd64 1.4.0-0ubuntu1~22.04.sav0 [17.0 kB]\n",
            "Get:25 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libwebpmux3 amd64 1.4.0-0ubuntu1~22.04.sav0 [30.2 kB]\n",
            "Get:26 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libwebpdecoder3 amd64 1.4.0-0ubuntu1~22.04.sav0 [115 kB]\n",
            "Get:27 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libsharpyuv0 amd64 1.4.0-0ubuntu1~22.04.sav0 [19.4 kB]\n",
            "Get:28 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libsharpyuv-dev amd64 1.4.0-0ubuntu1~22.04.sav0 [23.8 kB]\n",
            "Get:29 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libwebp7 amd64 1.4.0-0ubuntu1~22.04.sav0 [230 kB]\n",
            "Get:30 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libx264-164 amd64 2:0.164.3191+git4613ac3-0ubuntu1~22.04.sav0 [598 kB]\n",
            "Get:31 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 libx265-209 amd64 3.6-3~22.04.sav1 [1,265 kB]\n",
            "Fetched 32.7 MB in 51s (639 kB/s)\n",
            "Extracting templates from packages: 100%\n",
            "(Reading database ... 123597 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libavformat-dev_7%3a4.4.5-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libavformat-dev:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) over (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../01-libavcodec-dev_7%3a4.4.5-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libavcodec-dev:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) over (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../02-libswresample-dev_7%3a4.4.5-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libswresample-dev:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) over (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../03-ffmpeg_7%3a4.4.5-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking ffmpeg (7:4.4.5-0ubuntu1~22.04.sav0) over (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../04-libavdevice58_7%3a4.4.5-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libavdevice58:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) over (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../05-libavfilter7_7%3a4.4.5-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libavfilter7:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) over (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../06-libavformat58_7%3a4.4.5-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libavformat58:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) over (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../07-libavcodec58_7%3a4.4.5-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libavcodec58:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) over (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../08-libswresample3_7%3a4.4.5-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libswresample3:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) over (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../09-libpostproc55_7%3a4.4.5-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libpostproc55:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) over (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../10-libswscale-dev_7%3a4.4.5-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libswscale-dev:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) over (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../11-libswscale5_7%3a4.4.5-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libswscale5:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) over (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../12-libavutil-dev_7%3a4.4.5-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libavutil-dev:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) over (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../13-libavutil56_7%3a4.4.5-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libavutil56:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) over (7:4.4.2-0ubuntu0.22.04.1) ...\n",
            "Preparing to unpack .../14-libsndio7.0_1.9.0-0.3~22.04.sav0_amd64.deb ...\n",
            "Unpacking libsndio7.0:amd64 (1.9.0-0.3~22.04.sav0) over (1.8.1-1.1) ...\n",
            "Selecting previously unselected package libsndio7:amd64.\n",
            "Preparing to unpack .../15-libsndio7_1.9.0-0.3~22.04.sav0_amd64.deb ...\n",
            "Unpacking libsndio7:amd64 (1.9.0-0.3~22.04.sav0) ...\n",
            "Selecting previously unselected package libfftw3-double3:amd64.\n",
            "Preparing to unpack .../16-libfftw3-double3_3.3.8-2ubuntu8_amd64.deb ...\n",
            "Unpacking libfftw3-double3:amd64 (3.3.8-2ubuntu8) ...\n",
            "Preparing to unpack .../17-librubberband2_3.3.0+dfsg-1~22.04.sav0_amd64.deb ...\n",
            "Unpacking librubberband2:amd64 (3.3.0+dfsg-1~22.04.sav0) over (2.0.0-2) ...\n",
            "Selecting previously unselected package libsrt1.5-gnutls:amd64.\n",
            "Preparing to unpack .../18-libsrt1.5-gnutls_1.5.3-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libsrt1.5-gnutls:amd64 (1.5.3-0ubuntu1~22.04.sav0) ...\n",
            "Selecting previously unselected package libcodec2-1.2:amd64.\n",
            "Preparing to unpack .../19-libcodec2-1.2_1.2.0-2~22.04.sav0_amd64.deb ...\n",
            "Unpacking libcodec2-1.2:amd64 (1.2.0-2~22.04.sav0) ...\n",
            "Preparing to unpack .../20-libsnappy1v5_1.1.10-1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libsnappy1v5:amd64 (1.1.10-1~22.04.sav0) over (1.1.8-1build3) ...\n",
            "Selecting previously unselected package libvpx9:amd64.\n",
            "Preparing to unpack .../21-libvpx9_1.14.1-1ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libvpx9:amd64 (1.14.1-1ubuntu1~22.04.sav0) ...\n",
            "Preparing to unpack .../22-libwebp-dev_1.4.0-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libwebp-dev:amd64 (1.4.0-0ubuntu1~22.04.sav0) over (1.2.2-2ubuntu0.22.04.2) ...\n",
            "Preparing to unpack .../23-libwebpdemux2_1.4.0-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libwebpdemux2:amd64 (1.4.0-0ubuntu1~22.04.sav0) over (1.2.2-2ubuntu0.22.04.2) ...\n",
            "Preparing to unpack .../24-libwebpmux3_1.4.0-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libwebpmux3:amd64 (1.4.0-0ubuntu1~22.04.sav0) over (1.2.2-2ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package libwebpdecoder3:amd64.\n",
            "Preparing to unpack .../25-libwebpdecoder3_1.4.0-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libwebpdecoder3:amd64 (1.4.0-0ubuntu1~22.04.sav0) ...\n",
            "Selecting previously unselected package libsharpyuv0:amd64.\n",
            "Preparing to unpack .../26-libsharpyuv0_1.4.0-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libsharpyuv0:amd64 (1.4.0-0ubuntu1~22.04.sav0) ...\n",
            "Selecting previously unselected package libsharpyuv-dev:amd64.\n",
            "Preparing to unpack .../27-libsharpyuv-dev_1.4.0-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libsharpyuv-dev:amd64 (1.4.0-0ubuntu1~22.04.sav0) ...\n",
            "Preparing to unpack .../28-libwebp7_1.4.0-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libwebp7:amd64 (1.4.0-0ubuntu1~22.04.sav0) over (1.2.2-2ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package libx264-164:amd64.\n",
            "Preparing to unpack .../29-libx264-164_2%3a0.164.3191+git4613ac3-0ubuntu1~22.04.sav0_amd64.deb ...\n",
            "Unpacking libx264-164:amd64 (2:0.164.3191+git4613ac3-0ubuntu1~22.04.sav0) ...\n",
            "Selecting previously unselected package libx265-209:amd64.\n",
            "Preparing to unpack .../30-libx265-209_3.6-3~22.04.sav1_amd64.deb ...\n",
            "Unpacking libx265-209:amd64 (3.6-3~22.04.sav1) ...\n",
            "Setting up libsharpyuv0:amd64 (1.4.0-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libx264-164:amd64 (2:0.164.3191+git4613ac3-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libcodec2-1.2:amd64 (1.2.0-2~22.04.sav0) ...\n",
            "Setting up libwebpdecoder3:amd64 (1.4.0-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libavutil56:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libsnappy1v5:amd64 (1.1.10-1~22.04.sav0) ...\n",
            "Setting up libx265-209:amd64 (3.6-3~22.04.sav1) ...\n",
            "Setting up libpostproc55:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libfftw3-double3:amd64 (3.3.8-2ubuntu8) ...\n",
            "Setting up libwebp7:amd64 (1.4.0-0ubuntu1~22.04.sav0) ...\n",
            "Setting up librubberband2:amd64 (3.3.0+dfsg-1~22.04.sav0) ...\n",
            "Setting up libvpx9:amd64 (1.14.1-1ubuntu1~22.04.sav0) ...\n",
            "Setting up libsrt1.5-gnutls:amd64 (1.5.3-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libswscale5:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libsndio7:amd64 (1.9.0-0.3~22.04.sav0) ...\n",
            "Setting up libsharpyuv-dev:amd64 (1.4.0-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libwebpmux3:amd64 (1.4.0-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libavutil-dev:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libwebpdemux2:amd64 (1.4.0-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libswresample3:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libwebp-dev:amd64 (1.4.0-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libswscale-dev:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libsndio7.0:amd64 (1.9.0-0.3~22.04.sav0) ...\n",
            "Setting up libavcodec58:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libswresample-dev:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libavformat58:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libavcodec-dev:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libavformat-dev:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libavfilter7:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) ...\n",
            "Setting up libavdevice58:amd64 (7:4.4.5-0ubuntu1~22.04.sav0) ...\n",
            "Setting up ffmpeg (7:4.4.5-0ubuntu1~22.04.sav0) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev\n",
        "! sudo add-apt-repository ppa:savoury1/ffmpeg4 -y && apt-get update && apt-get install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f59VH15fy-RY",
      "metadata": {
        "id": "f59VH15fy-RY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "6d84a9d3-a68b-40a2-92ad-404310d3d1ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libboost-program-options-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libboost-program-options-dev set to manually installed.\n",
            "libboost-system-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libboost-system-dev set to manually installed.\n",
            "libboost-thread-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libboost-thread-dev set to manually installed.\n",
            "libbz2-dev is already the newest version (1.0.8-5build1).\n",
            "liblzma-dev is already the newest version (5.2.5-2ubuntu1).\n",
            "libzstd-dev is already the newest version (1.4.8+dfsg-3build1).\n",
            "libzstd-dev set to manually installed.\n",
            "libboost-test-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libboost-test-dev set to manually installed.\n",
            "libopenblas-dev is already the newest version (0.3.20+ds-1).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "libsndfile1-dev is already the newest version (1.0.31-2ubuntu0.1).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libcodec2-1.0 libmfx1 libsrt1.4-gnutls libva-drm2 libva-x11-2 libva2 libvpx7\n",
            "  libx264-163\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "The following additional packages will be installed:\n",
            "  libfftw3-bin libfftw3-long3 libfftw3-quad3 libfftw3-single3 libgflags2.2\n",
            "  libgoogle-glog0v5 libunwind-dev zlib1g\n",
            "Suggested packages:\n",
            "  libeigen3-doc libmpfrc++-dev libfftw3-doc\n",
            "The following NEW packages will be installed:\n",
            "  libeigen3-dev libfftw3-bin libfftw3-dev libfftw3-long3 libfftw3-quad3\n",
            "  libfftw3-single3 libgflags-dev libgflags2.2 libgoogle-glog-dev\n",
            "  libgoogle-glog0v5 libunwind-dev\n",
            "The following packages will be upgraded:\n",
            "  zlib1g zlib1g-dev\n",
            "2 upgraded, 11 newly installed, 0 to remove and 111 not upgraded.\n",
            "Need to get 8,186 kB of archives.\n",
            "After this operation, 39.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfftw3-long3 amd64 3.3.8-2ubuntu8 [335 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfftw3-quad3 amd64 3.3.8-2ubuntu8 [614 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfftw3-single3 amd64 3.3.8-2ubuntu8 [800 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfftw3-bin amd64 3.3.8-2ubuntu8 [35.5 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfftw3-dev amd64 3.3.8-2ubuntu8 [2,101 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgflags2.2 amd64 2.2.2-2 [78.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgflags-dev amd64 2.2.2-2 [93.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgoogle-glog0v5 amd64 0.5.0+really0.4.0-2 [60.3 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libunwind-dev amd64 1.3.2-2build2.1 [1,883 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 zlib1g-dev amd64 1:1.3.1.dfsg-0ubuntu1~22.04.sav1 [938 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgoogle-glog-dev amd64 0.5.0+really0.4.0-2 [91.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libeigen3-dev all 3.4.0-2ubuntu2 [1,056 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/savoury1/ffmpeg4/ubuntu jammy/main amd64 zlib1g amd64 1:1.3.1.dfsg-0ubuntu1~22.04.sav1 [101 kB]\n",
            "Fetched 8,186 kB in 3s (3,032 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 13.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 123675 files and directories currently installed.)\n",
            "Preparing to unpack .../zlib1g-dev_1%3a1.3.1.dfsg-0ubuntu1~22.04.sav1_amd64.deb ...\n",
            "Unpacking zlib1g-dev:amd64 (1:1.3.1.dfsg-0ubuntu1~22.04.sav1) over (1:1.2.11.dfsg-2ubuntu9.2) ...\n",
            "Preparing to unpack .../zlib1g_1%3a1.3.1.dfsg-0ubuntu1~22.04.sav1_amd64.deb ...\n",
            "Unpacking zlib1g:amd64 (1:1.3.1.dfsg-0ubuntu1~22.04.sav1) over (1:1.2.11.dfsg-2ubuntu9.2) ...\n",
            "Setting up zlib1g:amd64 (1:1.3.1.dfsg-0ubuntu1~22.04.sav1) ...\n",
            "Selecting previously unselected package libfftw3-long3:amd64.\n",
            "(Reading database ... 123680 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libfftw3-long3_3.3.8-2ubuntu8_amd64.deb ...\n",
            "Unpacking libfftw3-long3:amd64 (3.3.8-2ubuntu8) ...\n",
            "Selecting previously unselected package libfftw3-quad3:amd64.\n",
            "Preparing to unpack .../01-libfftw3-quad3_3.3.8-2ubuntu8_amd64.deb ...\n",
            "Unpacking libfftw3-quad3:amd64 (3.3.8-2ubuntu8) ...\n",
            "Selecting previously unselected package libfftw3-single3:amd64.\n",
            "Preparing to unpack .../02-libfftw3-single3_3.3.8-2ubuntu8_amd64.deb ...\n",
            "Unpacking libfftw3-single3:amd64 (3.3.8-2ubuntu8) ...\n",
            "Selecting previously unselected package libfftw3-bin.\n",
            "Preparing to unpack .../03-libfftw3-bin_3.3.8-2ubuntu8_amd64.deb ...\n",
            "Unpacking libfftw3-bin (3.3.8-2ubuntu8) ...\n",
            "Selecting previously unselected package libfftw3-dev:amd64.\n",
            "Preparing to unpack .../04-libfftw3-dev_3.3.8-2ubuntu8_amd64.deb ...\n",
            "Unpacking libfftw3-dev:amd64 (3.3.8-2ubuntu8) ...\n",
            "Selecting previously unselected package libgflags2.2.\n",
            "Preparing to unpack .../05-libgflags2.2_2.2.2-2_amd64.deb ...\n",
            "Unpacking libgflags2.2 (2.2.2-2) ...\n",
            "Selecting previously unselected package libgflags-dev.\n",
            "Preparing to unpack .../06-libgflags-dev_2.2.2-2_amd64.deb ...\n",
            "Unpacking libgflags-dev (2.2.2-2) ...\n",
            "Selecting previously unselected package libgoogle-glog0v5.\n",
            "Preparing to unpack .../07-libgoogle-glog0v5_0.5.0+really0.4.0-2_amd64.deb ...\n",
            "Unpacking libgoogle-glog0v5 (0.5.0+really0.4.0-2) ...\n",
            "Selecting previously unselected package libunwind-dev:amd64.\n",
            "Preparing to unpack .../08-libunwind-dev_1.3.2-2build2.1_amd64.deb ...\n",
            "Unpacking libunwind-dev:amd64 (1.3.2-2build2.1) ...\n",
            "Selecting previously unselected package libgoogle-glog-dev.\n",
            "Preparing to unpack .../09-libgoogle-glog-dev_0.5.0+really0.4.0-2_amd64.deb ...\n",
            "Unpacking libgoogle-glog-dev (0.5.0+really0.4.0-2) ...\n",
            "Selecting previously unselected package libeigen3-dev.\n",
            "Preparing to unpack .../10-libeigen3-dev_3.4.0-2ubuntu2_all.deb ...\n",
            "Unpacking libeigen3-dev (3.4.0-2ubuntu2) ...\n",
            "Setting up libfftw3-single3:amd64 (3.3.8-2ubuntu8) ...\n",
            "Setting up libunwind-dev:amd64 (1.3.2-2build2.1) ...\n",
            "Setting up libfftw3-long3:amd64 (3.3.8-2ubuntu8) ...\n",
            "Setting up libfftw3-quad3:amd64 (3.3.8-2ubuntu8) ...\n",
            "Setting up libeigen3-dev (3.4.0-2ubuntu2) ...\n",
            "Setting up zlib1g-dev:amd64 (1:1.3.1.dfsg-0ubuntu1~22.04.sav1) ...\n",
            "Setting up libgflags2.2 (2.2.2-2) ...\n",
            "Setting up libfftw3-bin (3.3.8-2ubuntu8) ...\n",
            "Setting up libgflags-dev (2.2.2-2) ...\n",
            "Setting up libfftw3-dev:amd64 (3.3.8-2ubuntu8) ...\n",
            "Setting up libgoogle-glog0v5 (0.5.0+really0.4.0-2) ...\n",
            "Setting up libgoogle-glog-dev (0.5.0+really0.4.0-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sudo apt install -y liblzma-dev libbz2-dev libzstd-dev libsndfile1-dev libopenblas-dev libfftw3-dev libgflags-dev libgoogle-glog-dev build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "df4673b7",
      "metadata": {
        "collapsed": true,
        "id": "df4673b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93857ec7-23d8-43d0-d127-550495231340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pyctcdecode\n",
            "  Downloading pyctcdecode-0.5.0-py2.py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Collecting pygtrie<3.0,>=2.1 (from pyctcdecode)\n",
            "  Downloading pygtrie-2.5.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting hypothesis<7,>=6.14 (from pyctcdecode)\n",
            "  Downloading hypothesis-6.112.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (1.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyctcdecode-0.5.0-py2.py3-none-any.whl (39 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hypothesis-6.112.0-py3-none-any.whl (467 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.5/467.5 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygtrie, xxhash, pyarrow, hypothesis, dill, pyctcdecode, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.0.0 dill-0.3.8 hypothesis-6.112.0 multiprocess-0.70.16 pyarrow-17.0.0 pyctcdecode-0.5.0 pygtrie-2.5.0 xxhash-3.5.0\n",
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Downloading https://github.com/kpu/kenlm/archive/master.zip (553 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.6/553.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.2.0-cp310-cp310-linux_x86_64.whl size=3184445 sha256=94fdc4d6cacaf584b6069e112601693e50683d816aa6c1ec3f5ab86045101bf4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s1dz5cls/wheels/a5/73/ee/670fbd0cee8f6f0b21d10987cb042291e662e26e1a07026462\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.2.0\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers datasets pyctcdecode soundfile\n",
        "! pip install https://github.com/kpu/kenlm/archive/master.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bXTdRmRd3V8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXTdRmRd3V8d",
        "outputId": "278e1c9d-76e8-4f0f-adc8-fe367d85b6fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "275c058e",
      "metadata": {
        "id": "275c058e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d6ad30bb-4f55-495a-98e7-baa20e41335f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IndicWav2Vec'...\n",
            "remote: Enumerating objects: 1943, done.\u001b[K\n",
            "remote: Counting objects: 100% (228/228), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 1943 (delta 158), reused 204 (delta 142), pack-reused 1715 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1943/1943), 139.60 MiB | 17.45 MiB/s, done.\n",
            "Resolving deltas: 100% (360/360), done.\n",
            "Updating files: 100% (759/759), done.\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 35337, done.\u001b[K\n",
            "remote: Counting objects: 100% (254/254), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 35337 (delta 104), reused 215 (delta 87), pack-reused 35083 (from 1)\u001b[K\n",
            "Receiving objects: 100% (35337/35337), 25.32 MiB | 12.56 MiB/s, done.\n",
            "Resolving deltas: 100% (25594/25594), done.\n",
            "Cloning into 'kenlm'...\n",
            "remote: Enumerating objects: 14170, done.\u001b[K\n",
            "remote: Counting objects: 100% (483/483), done.\u001b[K\n",
            "remote: Compressing objects: 100% (337/337), done.\u001b[K\n",
            "remote: Total 14170 (delta 166), reused 408 (delta 132), pack-reused 13687 (from 1)\u001b[K\n",
            "Receiving objects: 100% (14170/14170), 5.91 MiB | 7.30 MiB/s, done.\n",
            "Resolving deltas: 100% (8046/8046), done.\n",
            "Cloning into 'flashlight'...\n",
            "remote: Enumerating objects: 25960, done.\u001b[K\n",
            "remote: Counting objects: 100% (148/148), done.\u001b[K\n",
            "remote: Compressing objects: 100% (113/113), done.\u001b[K\n",
            "remote: Total 25960 (delta 55), reused 105 (delta 34), pack-reused 25812 (from 1)\u001b[K\n",
            "Receiving objects: 100% (25960/25960), 15.88 MiB | 12.06 MiB/s, done.\n",
            "Resolving deltas: 100% (18584/18584), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf IndicWav2Vec fairseq kenlm flashlight\n",
        "!git clone https://github.com/AI4Bharat/IndicWav2Vec.git\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "!git clone https://github.com/kpu/kenlm.git\n",
        "!git clone https://github.com/flashlight/flashlight.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<1.24\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "-ZoOZnTjrD8K"
      },
      "id": "-ZoOZnTjrD8K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pip==24.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NYbkd0f7s5kE",
        "outputId": "9740f2cb-a595-4c4d-aba0-dec80d415569"
      },
      "id": "NYbkd0f7s5kE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pip==24.0\n",
            "  Using cached pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Using cached pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build IndicWav2Vec"
      ],
      "metadata": {
        "id": "is_rEC4FrJ1q"
      },
      "id": "is_rEC4FrJ1q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔥 Remove `numpy==1.20.0` from `/content/IndicWav2Vecw2v_inference/requirements.txt` file 🔥"
      ],
      "metadata": {
        "id": "5UPevrBhv5cK"
      },
      "id": "5UPevrBhv5cK"
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/IndicWav2Vec\n",
        "!pip install packaging soundfile swifter -r w2v_inference/requirements.txt\n",
        "%cd .."
      ],
      "metadata": {
        "id": "8rc9zYRkrMec"
      },
      "id": "8rc9zYRkrMec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build Fairseq"
      ],
      "metadata": {
        "id": "bUncTBBEKity"
      },
      "id": "bUncTBBEKity"
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/fairseq\n",
        "!git checkout cf8ff8c3c5242e6e71e8feb40de45dd699f3cc08\n",
        "!pip install ./\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wJNJ5uMlKTv9",
        "outputId": "2e65a773-52af-48cf-f29d-269e095fa97d"
      },
      "id": "wJNJ5uMlKTv9",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fairseq\n",
            "Note: switching to 'cf8ff8c3c5242e6e71e8feb40de45dd699f3cc08'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at cf8ff8c3 Add unittests for jitting EMA model\n",
            "Processing /content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+cf8ff8c) (1.17.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+cf8ff8c) (3.0.11)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==1.0.0a0+cf8ff8c)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+cf8ff8c) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+cf8ff8c) (2024.5.15)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq==1.0.0a0+cf8ff8c)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+cf8ff8c) (2.4.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+cf8ff8c) (4.56.0)\n",
            "Collecting bitarray (from fairseq==1.0.0a0+cf8ff8c)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+cf8ff8c) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq==1.0.0a0+cf8ff8c) (1.23.5)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+cf8ff8c)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+cf8ff8c) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+cf8ff8c) (4.12.2)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq==1.0.0a0+cf8ff8c)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+cf8ff8c) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq==1.0.0a0+cf8ff8c)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+cf8ff8c) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+cf8ff8c) (3.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+cf8ff8c) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+cf8ff8c) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+cf8ff8c) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==1.0.0a0+cf8ff8c) (2024.6.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==1.0.0a0+cf8ff8c) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq==1.0.0a0+cf8ff8c) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq==1.0.0a0+cf8ff8c) (1.3.0)\n",
            "Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+cf8ff8c-cp310-cp310-linux_x86_64.whl size=4097452 sha256=d8d92ad282d32c9e69856840a1b94a452c95c315e818baf7e6e3efe1ec247ca0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u0_z28tg/wheels/c6/d7/db/bc419b1daa8266aa8de2a7c4d29f62dbfa814e8701fe4695a2\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141213 sha256=a4e781c1c830d4685eb54e10f1ff4a9953e2782431b750cb9869de4aa0cf9c1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: bitarray, antlr4-python3-runtime, portalocker, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-1.0.0a0+cf8ff8c hydra-core-1.0.7 portalocker-2.10.1 sacrebleu-2.4.3\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c920980b",
      "metadata": {
        "id": "c920980b"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "452d4bd4",
      "metadata": {
        "id": "452d4bd4"
      },
      "source": [
        "Download model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c3d3c5b9",
      "metadata": {
        "id": "c3d3c5b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fc72b58-7f1c-4c40-db03-2d0dc200c386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-12 13:17:03--  https://indic-asr-public.objectstore.e2enetworks.net/indic-superb/models/acoustic/kannada.pt\n",
            "Resolving indic-asr-public.objectstore.e2enetworks.net (indic-asr-public.objectstore.e2enetworks.net)... 164.52.210.97, 101.53.152.30, 164.52.206.154, ...\n",
            "Connecting to indic-asr-public.objectstore.e2enetworks.net (indic-asr-public.objectstore.e2enetworks.net)|164.52.210.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3786635828 (3.5G) [application/zip]\n",
            "Saving to: ‘/content/kn.pt’\n",
            "\n",
            "/content/kn.pt      100%[===================>]   3.53G  15.6MB/s    in 3m 47s  \n",
            "\n",
            "2024-09-12 13:20:53 (15.9 MB/s) - ‘/content/kn.pt’ saved [3786635828/3786635828]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://indic-asr-public.objectstore.e2enetworks.net/indic-superb/models/acoustic/kannada.pt -O /content/kn.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61O-Uw-zEchG",
      "metadata": {
        "id": "61O-Uw-zEchG"
      },
      "source": [
        "Install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "CypJ5WDdETZc",
      "metadata": {
        "id": "CypJ5WDdETZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b07bd81-62e7-40ab-ea48-f30db95854a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected operating system as Ubuntu/jammy.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Detected apt version as 2.4.13\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n",
            "done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libcodec2-1.0 libmfx1 libsrt1.4-gnutls libva-drm2 libva-x11-2 libva2 libvpx7 libx264-163\n",
            "Use 'apt autoremove' to remove them.\n",
            "The following packages will be upgraded:\n",
            "  git-lfs\n",
            "1 upgraded, 0 newly installed, 0 to remove and 111 not upgraded.\n",
            "Need to get 7,420 kB of archives.\n",
            "After this operation, 6,051 kB of additional disk space will be used.\n",
            "Get:1 https://packagecloud.io/github/git-lfs/ubuntu jammy/main amd64 git-lfs amd64 3.5.1 [7,420 kB]\n",
            "Fetched 7,420 kB in 0s (17.6 MB/s)\n",
            "(Reading database ... 124499 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_3.5.1_amd64.deb ...\n",
            "Unpacking git-lfs (3.5.1) over (3.0.2-1ubuntu0.2) ...\n",
            "Setting up git-lfs (3.5.1) ...\n",
            "Git LFS initialized.\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Git LFS initialized.\n"
          ]
        }
      ],
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\n",
        "!apt-get install git-lfs\n",
        "!git lfs install\n",
        "\n",
        "# Put in your details\n",
        "!git config --global user.email \"mishraaditya6991@gmail.com\"\n",
        "!git config --global user.name \"adimyth\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Nw8pONYUEeVY",
      "metadata": {
        "id": "Nw8pONYUEeVY"
      },
      "source": [
        "Login to huggingface-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7PUcFX61EbNa",
      "metadata": {
        "id": "7PUcFX61EbNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc29ee9-0459-4d57-9577-a373e7d25380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.29.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9abY4Lx914oU",
        "outputId": "5b785765-876e-4bdd-9dd6-493752502509"
      },
      "id": "9abY4Lx914oU",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.29.2\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl.metadata (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.29.2)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (4.56.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (2024.8.30)\n",
            "Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "transformers.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_YujB775162c",
        "outputId": "f7501e9d-3dcb-4b9c-ad34-aa0edc0f84db"
      },
      "id": "_YujB775162c",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.29.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "aiQ3Dn22TqgM",
      "metadata": {
        "id": "aiQ3Dn22TqgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f68fa6c-508f-4991-f693-5e39acef2fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-09-12 13:23:07.642641: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-12 13:23:08.019013: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-12 13:23:08.109697: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-12 13:23:12.786300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from transformers import Wav2Vec2Config\n",
        "from huggingface_hub import create_repo, Repository\n",
        "\n",
        "from transformers import pipeline, AutoModelForCTC, Wav2Vec2Processor, Wav2Vec2ProcessorWithLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b96f514",
      "metadata": {
        "id": "6b96f514"
      },
      "source": [
        "### Export models to HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vk6VHD2qDJfy",
      "metadata": {
        "id": "Vk6VHD2qDJfy"
      },
      "source": [
        "Create and Initialize Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "V9CL4xLqDHkg",
      "metadata": {
        "id": "V9CL4xLqDHkg"
      },
      "outputs": [],
      "source": [
        "repo_url = create_repo(\"indicwav2vec-kannada\", private=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O90xuEiFHtRo",
      "metadata": {
        "id": "O90xuEiFHtRo"
      },
      "source": [
        "Save config.json from a \"similar\" architecture in huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "F2-XWV-O_Pbo",
      "metadata": {
        "id": "F2-XWV-O_Pbo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fa213e-a272-4045-a9af-6d4c430231c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
            "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Cloning https://huggingface.co/adimyth/indicwav2vec-kannada into local empty directory.\n",
            "WARNING:huggingface_hub.repository:Cloning https://huggingface.co/adimyth/indicwav2vec-kannada into local empty directory.\n"
          ]
        }
      ],
      "source": [
        "repo = Repository(local_dir=\"indicwav2vec-kannada\", clone_from=repo_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "zknkfbX8HlBy",
      "metadata": {
        "id": "zknkfbX8HlBy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "4fb3fd48298b48b7b417c9b8d604a471",
            "91dd9d25e3c74c82abdfb8670a977ad0",
            "ae33c0fc51364b749d2fb4a187ba6b44",
            "bd442dfade2c450aa134092ab992aba1",
            "df5bf9a136a24db2b835b3bf9e49ab54",
            "9964aa88d34440bfbd0e071d36771363",
            "042e3c19b8e3404d87bca6fa4ad2b725",
            "d71e3056653649e4ac1f0d1814448e2a",
            "77d209d12c63410f8efbc335c5f2d749",
            "6ac6cdf7e2b64fc1a97b960f4ab2e243",
            "815d562937e644409e2a908c09356036"
          ]
        },
        "outputId": "ff47e26e-359d-43a0-8120-dbdb411f2fda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fb3fd48298b48b7b417c9b8d604a471"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "config = Wav2Vec2Config.from_pretrained('facebook/wav2vec2-large-960h-lv60-self')\n",
        "config.save_pretrained('indicwav2vec-kannada')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "aZGhr0T9_caw",
      "metadata": {
        "id": "aZGhr0T9_caw"
      },
      "outputs": [],
      "source": [
        "# using the indicwav2vec-hindi config.json for indicwav2vec-tamil\n",
        "import json\n",
        "\n",
        "data = {\n",
        "  \"_name_or_path\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
        "  \"activation_dropout\": 0.1,\n",
        "  \"adapter_kernel_size\": 3,\n",
        "  \"adapter_stride\": 2,\n",
        "  \"add_adapter\": False,\n",
        "  \"apply_spec_augment\": True,\n",
        "  \"architectures\": [\n",
        "    \"Wav2Vec2ForCTC\"\n",
        "  ],\n",
        "  \"attention_dropout\": 0.1,\n",
        "  \"bos_token_id\": 1,\n",
        "  \"classifier_proj_size\": 256,\n",
        "  \"codevector_dim\": 256,\n",
        "  \"contrastive_logits_temperature\": 0.1,\n",
        "  \"conv_bias\": True,\n",
        "  \"conv_dim\": [\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    512\n",
        "  ],\n",
        "  \"conv_kernel\": [\n",
        "    10,\n",
        "    3,\n",
        "    3,\n",
        "    3,\n",
        "    3,\n",
        "    2,\n",
        "    2\n",
        "  ],\n",
        "  \"conv_stride\": [\n",
        "    5,\n",
        "    2,\n",
        "    2,\n",
        "    2,\n",
        "    2,\n",
        "    2,\n",
        "    2\n",
        "  ],\n",
        "  \"ctc_loss_reduction\": \"sum\",\n",
        "  \"ctc_zero_infinity\": False,\n",
        "  \"diversity_loss_weight\": 0.1,\n",
        "  \"do_stable_layer_norm\": True,\n",
        "  \"eos_token_id\": 2,\n",
        "  \"feat_extract_activation\": \"gelu\",\n",
        "  \"feat_extract_dropout\": 0.0,\n",
        "  \"feat_extract_norm\": \"layer\",\n",
        "  \"feat_proj_dropout\": 0.1,\n",
        "  \"feat_quantizer_dropout\": 0.0,\n",
        "  \"final_dropout\": 0.1,\n",
        "  \"gradient_checkpointing\": False,\n",
        "  \"hidden_act\": \"gelu\",\n",
        "  \"hidden_dropout\": 0.1,\n",
        "  \"hidden_dropout_prob\": 0.1,\n",
        "  \"hidden_size\": 1024,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 4096,\n",
        "  \"layer_norm_eps\": 1e-05,\n",
        "  \"layerdrop\": 0.1,\n",
        "  \"mask_feature_length\": 10,\n",
        "  \"mask_feature_min_masks\": 0,\n",
        "  \"mask_feature_prob\": 0.0,\n",
        "  \"mask_time_length\": 10,\n",
        "  \"mask_time_min_masks\": 2,\n",
        "  \"mask_time_prob\": 0.05,\n",
        "  \"model_type\": \"wav2vec2\",\n",
        "  \"num_adapter_layers\": 3,\n",
        "  \"num_attention_heads\": 16,\n",
        "  \"num_codevector_groups\": 2,\n",
        "  \"num_codevectors_per_group\": 320,\n",
        "  \"num_conv_pos_embedding_groups\": 16,\n",
        "  \"num_conv_pos_embeddings\": 128,\n",
        "  \"num_feat_extract_layers\": 7,\n",
        "  \"num_hidden_layers\": 24,\n",
        "  \"num_negatives\": 100,\n",
        "  \"output_hidden_size\": 1024,\n",
        "  \"pad_token_id\": 0,\n",
        "  \"proj_codevector_dim\": 256,\n",
        "  \"tdnn_dilation\": [\n",
        "    1,\n",
        "    2,\n",
        "    3,\n",
        "    1,\n",
        "    1\n",
        "  ],\n",
        "  \"tdnn_dim\": [\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    1500\n",
        "  ],\n",
        "  \"tdnn_kernel\": [\n",
        "    5,\n",
        "    3,\n",
        "    3,\n",
        "    1,\n",
        "    1\n",
        "  ],\n",
        "  \"torch_dtype\": \"float32\",\n",
        "  \"transformers_version\": \"4.19.2\",\n",
        "  \"use_weighted_layer_sum\": False,\n",
        "  \"vocab_size\": 68,\n",
        "  \"xvector_output_dim\": 512\n",
        "}\n",
        "\n",
        "with open('/content/config.json', 'w') as f:\n",
        "    json.dump(data, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ybj-kkOTAAlk",
      "metadata": {
        "id": "ybj-kkOTAAlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd711dbe-045a-4a15-f1f1-dcb3973d032f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-12 13:24:17--  https://indic-asr-public.objectstore.e2enetworks.net/indic-superb/models/acoustic/kannada.dict.txt\n",
            "Resolving indic-asr-public.objectstore.e2enetworks.net (indic-asr-public.objectstore.e2enetworks.net)... 164.52.210.97, 101.53.152.30, 164.52.206.154, ...\n",
            "Connecting to indic-asr-public.objectstore.e2enetworks.net (indic-asr-public.objectstore.e2enetworks.net)|164.52.210.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 457 [text/plain]\n",
            "Saving to: ‘/content/dict.ltr.txt’\n",
            "\n",
            "/content/dict.ltr.t 100%[===================>]     457  --.-KB/s    in 0s      \n",
            "\n",
            "2024-09-12 13:24:19 (298 MB/s) - ‘/content/dict.ltr.txt’ saved [457/457]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# downloading the dictionary as per the github repo\n",
        "!wget https://indic-asr-public.objectstore.e2enetworks.net/indic-superb/models/acoustic/kannada.dict.txt -O /content/dict.ltr.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oFwGI_G6zbiB",
      "metadata": {
        "id": "oFwGI_G6zbiB"
      },
      "source": [
        "Convert ASR model to Huggingface's format"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Override the `merge` function in `/usr/local/lib/python3.10/dist-packages/omegaconf/omegaconf.py` with the following -\n",
        "\n",
        "```python\n",
        "    def merge(\n",
        "        *others: Union[BaseContainer, Dict[str, Any], List[Any], Tuple[Any, ...], Any]\n",
        "    ) -> Union[ListConfig, DictConfig]:\n",
        "        \"\"\"Merge a list of previously created configs into a single one\"\"\"\n",
        "        assert len(others) > 0\n",
        "        target = copy.deepcopy(others[0])\n",
        "        target = _ensure_container(target)\n",
        "        assert isinstance(target, (DictConfig, ListConfig))\n",
        "\n",
        "\n",
        "        print(\"=\"*90)\n",
        "        print(f\"Target: {type(target)}\")\n",
        "        print(target)\n",
        "        print(\"=\"*90)\n",
        "\n",
        "        print(\"=\"*90)\n",
        "        print(f\"Others: {type(*others[1:])}\")\n",
        "        print(*others[1:])\n",
        "        print(\"=\"*90)\n",
        "\n",
        "        # with flag_override(target, \"readonly\", False):\n",
        "        #     target.merge_with(*others[1:])\n",
        "        #     turned_readonly = target._get_flag(\"readonly\") is True\n",
        "\n",
        "        with flag_override(target, \"readonly\", False):\n",
        "            if 'eval_wer_config' in target:\n",
        "              OmegaConf.set_struct(target.eval_wer_config, False)  # Allow adding new keys\n",
        "              \n",
        "              if 'eos_token' not in target.eval_wer_config:\n",
        "                  target.eval_wer_config.eos_token = \"</s>\"  # or whatever default you want\n",
        "            \n",
        "            # Proceed with the merge\n",
        "            target.merge_with(*others[1:])\n",
        "            \n",
        "\n",
        "            # Optionally, re-enable struct to lock the structure\n",
        "            if 'eval_wer_config' in target:\n",
        "              OmegaConf.set_struct(target.eval_wer_config, True)\n",
        "\n",
        "\n",
        "            turned_readonly = target._get_flag(\"readonly\") is True\n",
        "\n",
        "        if turned_readonly:\n",
        "            OmegaConf.set_readonly(target, True)\n",
        "\n",
        "        return target\n",
        "```"
      ],
      "metadata": {
        "id": "RMJcLmIyiJCf"
      },
      "id": "RMJcLmIyiJCf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e2e2612",
      "metadata": {
        "id": "8e2e2612",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14435228-322e-4778-8792-3d76c16ded9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/IndicWav2Vec\n",
            "2024-09-12 14:00:46.753223: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-12 14:00:46.783555: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-12 14:00:46.792464: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-12 14:00:49.677583: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading configuration file /content/config.json\n",
            "Model config Wav2Vec2Config {\n",
            "  \"_name_or_path\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"adapter_kernel_size\": 3,\n",
            "  \"adapter_stride\": 2,\n",
            "  \"add_adapter\": false,\n",
            "  \"apply_spec_augment\": true,\n",
            "  \"architectures\": [\n",
            "    \"Wav2Vec2ForCTC\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"codevector_dim\": 256,\n",
            "  \"contrastive_logits_temperature\": 0.1,\n",
            "  \"conv_bias\": true,\n",
            "  \"conv_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"conv_kernel\": [\n",
            "    10,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"conv_stride\": [\n",
            "    5,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"ctc_loss_reduction\": \"sum\",\n",
            "  \"ctc_zero_infinity\": false,\n",
            "  \"diversity_loss_weight\": 0.1,\n",
            "  \"do_stable_layer_norm\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"feat_extract_activation\": \"gelu\",\n",
            "  \"feat_extract_dropout\": 0.0,\n",
            "  \"feat_extract_norm\": \"layer\",\n",
            "  \"feat_proj_dropout\": 0.1,\n",
            "  \"feat_quantizer_dropout\": 0.0,\n",
            "  \"final_dropout\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.1,\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"model_type\": \"wav2vec2\",\n",
            "  \"num_adapter_layers\": 3,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_codevector_groups\": 2,\n",
            "  \"num_codevectors_per_group\": 320,\n",
            "  \"num_conv_pos_embedding_groups\": 16,\n",
            "  \"num_conv_pos_embeddings\": 128,\n",
            "  \"num_feat_extract_layers\": 7,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_negatives\": 100,\n",
            "  \"output_hidden_size\": 1024,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"proj_codevector_dim\": 256,\n",
            "  \"tdnn_dilation\": [\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"tdnn_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    1500\n",
            "  ],\n",
            "  \"tdnn_kernel\": [\n",
            "    5,\n",
            "    3,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 68,\n",
            "  \"xvector_output_dim\": 512\n",
            "}\n",
            "\n",
            "Feature extractor saved in /content/indicwav2vec-kannada/preprocessor_config.json\n",
            "tokenizer config file saved in /content/indicwav2vec-kannada/tokenizer_config.json\n",
            "Special tokens file saved in /content/indicwav2vec-kannada/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/checkpoint_utils.py:313: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "==========================================================================================\n",
            "Target: <class 'omegaconf.dictconfig.DictConfig'>\n",
            "{'_name': None, 'data': '???', 'labels': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_sample_size': None, 'min_sample_size': None, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'tpu': '${common.tpu}', 'text_compression_level': none, 'eval_wer': False, 'eval_wer_config': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_wer_tokenizer': None, 'eval_wer_post_process': 'letter', 'eval_bleu': False, 'eval_bleu_detok': None, 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_args': '{}', 'eval_bleu_print_samples': False, 'autoregressive': False}\n",
            "==========================================================================================\n",
            "==========================================================================================\n",
            "Others: <class 'omegaconf.dictconfig.DictConfig'>\n",
            "{'_name': 'audio_finetuning', 'data': '/content', 'labels': 'ltr', 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_sample_size': None, 'min_sample_size': None, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'tpu': False, 'text_compression_level': 'none', 'eval_wer': False, 'eval_wer_config': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_wer_tokenizer': None, 'eval_wer_post_process': 'letter', 'eval_bleu': False, 'eval_bleu_detok': None, 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_args': '{}', 'eval_bleu_print_samples': False, 'autoregressive': False}\n",
            "==========================================================================================\n",
            "==========================================================================================\n",
            "Target: <class 'omegaconf.dictconfig.DictConfig'>\n",
            "{'_name': None, 'w2v_path': '???', 'no_pretrained_weights': False, 'dropout_input': 0.0, 'final_dropout': 0.0, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'encoder_embed_dim': 768, 'apply_mask': False, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'freeze_finetune_updates': 0, 'feature_grad_mult': 0.0, 'layerdrop': 0.0, 'mask_channel_min_space': 1, 'mask_channel_before': False, 'normalize': '${task.normalize}', 'data': '${task.data}', 'w2v_args': None, 'checkpoint_activations': False, 'offload_activations': False, 'min_params_to_wrap': 100000000, 'ddp_backend': '${distributed_training.ddp_backend}', 'blank_weight': 0.0, 'blank_mode': 'add'}\n",
            "==========================================================================================\n",
            "==========================================================================================\n",
            "Others: <class 'omegaconf.dictconfig.DictConfig'>\n",
            "{'_name': 'wav2vec_ctc', 'w2v_path': '/nlsasfs/home/ai4bharat/gramesh/speechteam/indic-wav2vec-project/checkpoints/pretraining/checkpoint_ft.pt', 'no_pretrained_weights': False, 'dropout_input': 0.0, 'final_dropout': 0.0, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'encoder_embed_dim': 768, 'apply_mask': True, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 64, 'mask_channel_prob': 0.25, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'freeze_finetune_updates': 100, 'feature_grad_mult': 0.0, 'layerdrop': 0.1, 'mask_channel_min_space': 1, 'mask_channel_before': False, 'normalize': True, 'data': '/content', 'w2v_args': {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'tqdm', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'pretrain-v0.1', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/lusnlsas/paramsiddhi/iitm/yashkm/indic-wav2vec2/custom_task', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 24, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://dgxn21-cn:9269', 'distributed_port': 9269, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1200000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'assamese_train,bodo_train,gujarati_train,karbi_train,ladakhi_train,marathi_train,pahari_train,santali_train,urdu_train,balti_train,chhatisgar_train,hindi_train,kashmiri_train,lepcha_train,mizo_train,punjabi_train,sindhi_train,bengali_train,dogri_train,indian_english_train,khasi_train,maithili_train,nagamese_train,rajasthani_train,tamil_train,bhojpuri_train,garo_train,jaintia_train,kokborok_train,malayalam_train,nepali_train,sambalpuri_train,telugu_train,bhutia_train,gojri_train,kannada_train,konkani_train,manipuri_train,odia_train,sanskrit_train', 'valid_subset': 'assamese_valid,bodo_valid,gujarati_valid,karbi_valid,ladakhi_valid,marathi_valid,pahari_valid,santali_valid,urdu_valid,balti_valid,chhatisgar_valid,hindi_valid,kashmiri_valid,lepcha_valid,mizo_valid,punjabi_valid,sindhi_valid,bengali_valid,dogri_valid,indian_english_valid,khasi_valid,maithili_valid,nagamese_valid,rajasthani_valid,tamil_valid,bhojpuri_valid,garo_valid,jaintia_valid,kokborok_valid,malayalam_valid,nepali_valid,sambalpuri_valid,telugu_valid,bhutia_valid,gojri_valid,kannada_valid,konkani_valid,manipuri_valid,odia_valid,sanskrit_valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1200000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 1000000, 'stop_time_hours': 0.0, 'clip_norm': 0.5, 'sentence_avg': False, 'update_freq': [6], 'lr': [0.005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/lusnlsas/paramsiddhi/iitm/yashkm/pretrained_checkpoints/wav2vec-large-t0.7-final', 'restore_file': '/lusnlsas/paramsiddhi/iitm/yashkm/wav2vec-large/wav2vec_vox_new.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 5, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 24}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.1, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 768, 'layer_norm_first': True, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': True, 'logit_temp': 0.1, 'quantize_targets': True, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 0.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 64, 'mask_channel_prob': 0.25, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.1, 0.999995]}, 'task': {'_name': 'audio_finetuning', 'data': '/content', 'max_sample_size': 320000, 'min_sample_size': 32000, 'normalize': True, 'sampling_alpha': 0.7}, 'criterion': None, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [0.005]}, 'lr_scheduler': None, 'scoring': None, 'bpe': None, 'tokenizer': None, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}, 'offload_activations': False, 'min_params_to_wrap': 100000000, 'checkpoint_activations': False, 'ddp_backend': 'legacy_ddp', 'blank_weight': 0.0, 'blank_mode': 'add'}\n",
            "==========================================================================================\n",
            "==========================================================================================\n",
            "Target: <class 'omegaconf.dictconfig.DictConfig'>\n",
            "{'_name': None, 'data': '???', 'labels': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_sample_size': None, 'min_sample_size': None, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'tpu': '${common.tpu}', 'text_compression_level': none, 'eval_wer': False, 'eval_wer_config': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_wer_tokenizer': None, 'eval_wer_post_process': 'letter', 'eval_bleu': False, 'eval_bleu_detok': None, 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_args': '{}', 'eval_bleu_print_samples': False, 'autoregressive': False}\n",
            "==========================================================================================\n",
            "==========================================================================================\n",
            "Others: <class 'omegaconf.dictconfig.DictConfig'>\n",
            "{'_name': 'audio_finetuning', 'data': '/content', 'max_sample_size': 320000, 'min_sample_size': 32000, 'normalize': True}\n",
            "==========================================================================================\n",
            "==========================================================================================\n",
            "Target: <class 'omegaconf.dictconfig.DictConfig'>\n",
            "{'_name': None, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n",
            "==========================================================================================\n",
            "==========================================================================================\n",
            "Others: <class 'omegaconf.dictconfig.DictConfig'>\n",
            "{'_name': 'wav2vec2', 'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.1, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 768, 'layer_norm_first': True, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': True, 'logit_temp': 0.1, 'quantize_targets': True, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 0.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 64, 'mask_channel_prob': 0.25, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.1, 0.999995]}\n",
            "==========================================================================================\n",
            "2024-09-12 14:01:22 | INFO | __main__ |  was initialized from w2v_encoder.w2v_model.mask_emb.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract conv layer 0 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract conv layer 0 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract layer norm weight of layer 0 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.0.2.1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract layer norm weight of layer 0 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.0.2.1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract conv layer 1 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.1.0.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract conv layer 1 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract layer norm weight of layer 1 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.1.2.1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract layer norm weight of layer 1 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.1.2.1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract conv layer 2 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.2.0.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract conv layer 2 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract layer norm weight of layer 2 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.2.2.1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract layer norm weight of layer 2 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.2.2.1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract conv layer 3 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.3.0.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract conv layer 3 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract layer norm weight of layer 3 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.3.2.1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract layer norm weight of layer 3 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.3.2.1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract conv layer 4 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.4.0.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract conv layer 4 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract layer norm weight of layer 4 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.4.2.1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract layer norm weight of layer 4 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.4.2.1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract conv layer 5 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.5.0.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract conv layer 5 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract layer norm weight of layer 5 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.5.2.1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract layer norm weight of layer 5 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.5.2.1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract conv layer 6 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.6.0.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract conv layer 6 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract layer norm weight of layer 6 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.6.2.1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | Feat extract layer norm weight of layer 6 was initialized from w2v_encoder.w2v_model.feature_extractor.conv_layers.6.2.1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.feature_projection.projection.weight was initialized from w2v_encoder.w2v_model.post_extract_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.feature_projection.projection.bias was initialized from w2v_encoder.w2v_model.post_extract_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.pos_conv_embed.conv.bias was initialized from w2v_encoder.w2v_model.encoder.pos_conv.0.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.pos_conv_embed.conv.weight_g was initialized from w2v_encoder.w2v_model.encoder.pos_conv.0.weight_g.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.pos_conv_embed.conv.weight_v was initialized from w2v_encoder.w2v_model.encoder.pos_conv.0.weight_v.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.0.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.0.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.0.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.0.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.0.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.0.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.0.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.0.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.0.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.0.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.0.fc1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.0.fc1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.0.fc2.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.0.fc2.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.0.final_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.0.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.0.final_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.1.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.1.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.1.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.1.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.1.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.1.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.1.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.1.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.1.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.1.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.1.fc1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.1.fc1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.1.fc2.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.1.fc2.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.1.final_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.1.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.1.final_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.2.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.2.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.2.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.2.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.2.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.2.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.2.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.2.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.2.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.2.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.2.fc1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.2.fc1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.2.fc2.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.2.fc2.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.2.final_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.2.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.2.final_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.3.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.3.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.3.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.3.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.3.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.3.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.3.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.3.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.3.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.3.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.3.fc1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.3.fc1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.3.fc2.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.3.fc2.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.3.final_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.3.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.3.final_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.4.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.4.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.4.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.4.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.4.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.4.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.4.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.4.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.4.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.4.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.4.fc1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.4.fc1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.4.fc2.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.4.fc2.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.4.final_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.4.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.4.final_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.5.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.5.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.5.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.5.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.5.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.5.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.5.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.5.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.5.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.5.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.5.fc1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.5.fc1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.5.fc2.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.5.fc2.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.5.final_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.5.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.5.final_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.6.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.6.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.6.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.6.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.6.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.6.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.6.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.6.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.6.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.6.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.6.fc1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.6.fc1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.6.fc2.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.6.fc2.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.6.final_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.6.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.6.final_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.7.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.7.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.7.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.7.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.7.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.7.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.7.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.7.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.7.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.7.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.7.fc1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.7.fc1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.7.fc2.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.7.fc2.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.7.final_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.7.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.7.final_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.8.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.8.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.8.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.8.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.8.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.8.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.8.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.8.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.8.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.8.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.8.fc1.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.8.fc1.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.8.fc2.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.8.fc2.bias.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.8.final_layer_norm.weight.\n",
            "2024-09-12 14:01:22 | INFO | __main__ | wav2vec2.encoder.layers.8.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.8.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.9.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.9.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.9.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.9.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.9.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.9.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.9.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.9.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.9.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.9.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.9.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.9.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.9.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.9.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.9.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.9.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.9.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.10.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.10.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.10.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.10.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.10.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.10.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.10.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.10.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.10.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.10.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.10.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.10.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.10.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.10.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.10.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.10.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.10.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.11.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.11.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.11.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.11.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.11.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.11.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.11.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.11.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.11.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.11.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.11.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.11.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.11.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.11.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.11.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.11.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.11.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.12.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.12.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.12.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.12.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.12.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.12.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.12.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.12.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.12.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.12.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.12.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.12.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.12.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.12.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.12.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.12.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.12.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.13.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.13.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.13.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.13.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.13.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.13.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.13.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.13.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.13.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.13.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.13.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.13.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.13.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.13.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.13.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.13.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.13.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.14.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.14.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.14.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.14.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.14.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.14.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.14.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.14.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.14.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.14.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.14.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.14.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.14.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.14.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.14.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.14.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.14.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.15.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.15.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.15.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.15.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.15.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.15.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.15.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.15.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.15.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.15.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.15.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.15.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.15.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.15.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.15.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.15.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.15.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.16.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.16.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.16.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.16.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.16.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.16.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.16.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.16.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.16.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.16.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.16.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.16.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.16.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.16.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.16.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.16.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.16.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.17.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.17.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.17.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.17.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.17.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.17.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.17.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.17.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.17.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.17.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.17.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.17.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.17.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.17.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.17.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.17.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.17.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.18.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.18.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.18.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.18.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.18.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.18.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.18.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.18.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.18.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.18.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.18.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.18.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.18.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.18.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.18.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.18.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.18.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.19.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.19.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.19.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.19.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.19.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.19.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.19.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.19.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.19.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.19.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.19.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.19.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.19.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.19.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.19.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.19.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.19.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.20.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.20.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.20.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.20.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.20.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.20.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.20.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.20.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.20.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.20.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.20.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.20.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.20.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.20.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.20.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.20.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.20.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.21.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.21.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.21.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.21.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.21.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.21.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.21.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.21.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.21.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.21.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.21.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.21.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.21.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.21.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.21.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.21.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.21.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.22.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.22.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.22.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.22.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.22.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.22.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.22.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.22.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.22.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.22.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.22.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.22.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.22.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.22.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.22.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.22.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.22.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.attention.k_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.23.self_attn.k_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.attention.k_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.23.self_attn.k_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.attention.v_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.23.self_attn.v_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.attention.v_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.23.self_attn.v_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.attention.q_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.23.self_attn.q_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.attention.q_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.23.self_attn.q_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.attention.out_proj.weight was initialized from w2v_encoder.w2v_model.encoder.layers.23.self_attn.out_proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.attention.out_proj.bias was initialized from w2v_encoder.w2v_model.encoder.layers.23.self_attn.out_proj.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.23.self_attn_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.23.self_attn_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.feed_forward.intermediate_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.23.fc1.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.feed_forward.intermediate_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.23.fc1.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.feed_forward.output_dense.weight was initialized from w2v_encoder.w2v_model.encoder.layers.23.fc2.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.feed_forward.output_dense.bias was initialized from w2v_encoder.w2v_model.encoder.layers.23.fc2.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.final_layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layers.23.final_layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layers.23.final_layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layers.23.final_layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layer_norm.weight was initialized from w2v_encoder.w2v_model.encoder.layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.encoder.layer_norm.bias was initialized from w2v_encoder.w2v_model.encoder.layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.feature_projection.layer_norm.weight was initialized from w2v_encoder.w2v_model.layer_norm.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | wav2vec2.feature_projection.layer_norm.bias was initialized from w2v_encoder.w2v_model.layer_norm.bias.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | lm_head.weight was initialized from w2v_encoder.proj.weight.\n",
            "2024-09-12 14:01:23 | INFO | __main__ | lm_head.bias was initialized from w2v_encoder.proj.bias.\n",
            "2024-09-12 14:01:23 | WARNING | __main__ | Unused weights: []\n",
            "Configuration saved in /content/indicwav2vec-kannada/config.json\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/IndicWav2Vec\"\n",
        "!python workshop-2022/utils/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py \\\n",
        "    --pytorch_dump_folder /content/indicwav2vec-kannada \\\n",
        "    --checkpoint_path /content/kn.pt \\\n",
        "    --config_path /content/config.json \\\n",
        "    --dict_path /content/dict.ltr.txt\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g6DhfRGt3BCL",
      "metadata": {
        "id": "g6DhfRGt3BCL"
      },
      "source": [
        "Push to Huggingface Model Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "GeAhCcF-31Wg",
      "metadata": {
        "id": "GeAhCcF-31Wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ddb162d-2afa-4721-d4f4-8f6d3b448b58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/indicwav2vec-kannada\n",
            "Local repo set up for largefiles\n",
            "Tracking \"*.binary\"\n",
            "[main 610171e] added language model\n",
            " 7 files changed, 215 insertions(+)\n",
            " create mode 100644 config.json\n",
            " create mode 100644 preprocessor_config.json\n",
            " create mode 100644 pytorch_model.bin\n",
            " create mode 100644 special_tokens_map.json\n",
            " create mode 100644 tokenizer_config.json\n",
            " create mode 100644 vocab.json\n",
            "Uploading LFS objects: 100% (1/1), 1.3 GB | 39 MB/s, done.\n",
            "Enumerating objects: 11, done.\n",
            "Counting objects: 100% (11/11), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 2.14 KiB | 2.14 MiB/s, done.\n",
            "Total 9 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "To https://huggingface.co/adimyth/indicwav2vec-kannada\n",
            "   7b70ce4..610171e  main -> main\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/indicwav2vec-kannada\"\n",
        "!huggingface-cli lfs-enable-largefiles .\n",
        "!git lfs track \"*.binary\"\n",
        "!git add .\n",
        "!git commit -m \"added language model\"\n",
        "!git push origin main\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yo7JUwwWId6i"
      },
      "id": "yo7JUwwWId6i",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "dfadc5c3dccc3fdd4ade829fe537ebc226dc9ee3485e5b880e2d3ccf90046cba"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4fb3fd48298b48b7b417c9b8d604a471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91dd9d25e3c74c82abdfb8670a977ad0",
              "IPY_MODEL_ae33c0fc51364b749d2fb4a187ba6b44",
              "IPY_MODEL_bd442dfade2c450aa134092ab992aba1"
            ],
            "layout": "IPY_MODEL_df5bf9a136a24db2b835b3bf9e49ab54"
          }
        },
        "91dd9d25e3c74c82abdfb8670a977ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9964aa88d34440bfbd0e071d36771363",
            "placeholder": "​",
            "style": "IPY_MODEL_042e3c19b8e3404d87bca6fa4ad2b725",
            "value": "config.json: 100%"
          }
        },
        "ae33c0fc51364b749d2fb4a187ba6b44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d71e3056653649e4ac1f0d1814448e2a",
            "max": 1606,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77d209d12c63410f8efbc335c5f2d749",
            "value": 1606
          }
        },
        "bd442dfade2c450aa134092ab992aba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ac6cdf7e2b64fc1a97b960f4ab2e243",
            "placeholder": "​",
            "style": "IPY_MODEL_815d562937e644409e2a908c09356036",
            "value": " 1.61k/1.61k [00:00&lt;00:00, 21.9kB/s]"
          }
        },
        "df5bf9a136a24db2b835b3bf9e49ab54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9964aa88d34440bfbd0e071d36771363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "042e3c19b8e3404d87bca6fa4ad2b725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d71e3056653649e4ac1f0d1814448e2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77d209d12c63410f8efbc335c5f2d749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ac6cdf7e2b64fc1a97b960f4ab2e243": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "815d562937e644409e2a908c09356036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}